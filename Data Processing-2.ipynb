{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Data Processing Stage-2 [Preparing Tefla ready data]\n",
    "\n",
    "This Notebook runs after the DataProcessing-1.ipynb notebook. This notebook creates the csv file with consists of image name and its corresponding label. Then it prepares the data in the tefla ready format, which the adequate distribution of data between training, validation, and test for all the categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ec2-user/src/tefla'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import glob\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# script to create a tefla compatible data dir for training and validation data\n",
    "sourceDir = '../../data/clothing/'\n",
    "destDir = '../../data/clothing/koovs_tefla_ready/'\n",
    "validation_data_percentage = 10\n",
    "test_data_percentage = 10\n",
    "categories = range(24)\n",
    "def create_tefla_data(source_dir,destination_dir,validation_percentage,test_percentage,categories):\n",
    "    if os.path.exists(destination_dir):\n",
    "        shutil.rmtree(destination_dir)\n",
    "\n",
    "    training_dir = destination_dir + 'training_224/'\n",
    "    validation_dir = destination_dir + 'validation_224/'\n",
    "    test_dir = destination_dir + 'test_224/'\n",
    "    os.makedirs(training_dir)\n",
    "    os.makedirs(validation_dir)\n",
    "    os.makedirs(test_dir)\n",
    "\n",
    "    labels = pd.read_csv( source_dir + 'all_koovs_new.csv')\n",
    "    validation_dict = {}\n",
    "    test_dict = {}\n",
    "\n",
    "    for c in categories:\n",
    "        validation_dict[c] = []\n",
    "\n",
    "    for c in categories:\n",
    "        test_dict[c] = []\n",
    "\n",
    "\n",
    "    # test set creation\n",
    "    test_set = []\n",
    "\n",
    "    for i, rows in labels.iterrows():\n",
    "        if test_dict.has_key(rows['labels']):\n",
    "            test_dict[rows['labels']].append(rows['image'])\n",
    "\n",
    "    for k in test_dict:\n",
    "        np.random.seed(0)\n",
    "        n = math.ceil(test_percentage * len(test_dict[k]) / 100.0)\n",
    "        random_array = np.random.choice(test_dict[k],int(n))\n",
    "        test_set = test_set + random_array.tolist()\n",
    "\n",
    "\n",
    "    #validation set creation\n",
    "    validation_set = []\n",
    "\n",
    "    for i, rows in labels.iterrows():\n",
    "        if rows['image'] not in test_set:\n",
    "            if validation_dict.has_key(rows['labels']):\n",
    "                validation_dict[rows['labels']].append(rows['image'])\n",
    "\n",
    "    for l in validation_dict:\n",
    "        np.random.seed(0)\n",
    "        n = math.ceil(validation_percentage * len(validation_dict[l]) / 100.0)\n",
    "        random_array = np.random.choice(validation_dict[l],int(n))\n",
    "        validation_set = validation_set + random_array.tolist()\n",
    "\n",
    "    training_images = []\n",
    "    training_labels = []\n",
    "    validation_images = []\n",
    "    validation_labels =[]\n",
    "    test_images = []\n",
    "    test_labels = []\n",
    "\n",
    "    for i, rows in labels.iterrows():\n",
    "        new_name = rows['image']\n",
    "        new_label = rows['labels']\n",
    "\n",
    "        if rows['labels'] == 2 :\n",
    "            new_label = 1\n",
    "\n",
    "        if rows['image'] in test_set:\n",
    "            test_images.append(new_name)\n",
    "            test_labels.append(new_label)\n",
    "            process_and_save_image(sourceDir + 'koovs/' + rows['image'] + \".jpg\", test_dir + new_name + \".jpg\");\n",
    "        elif rows['image'] in validation_set:\n",
    "            validation_images.append(new_name)\n",
    "            validation_labels.append(new_label)\n",
    "            process_and_save_image(sourceDir + 'koovs/' + rows['image'] + \".jpg\", validation_dir + new_name + \".jpg\")\n",
    "        else:\n",
    "            training_images.append(new_name)\n",
    "            training_labels.append(new_label)\n",
    "            process_and_save_image(sourceDir + 'koovs/' + rows['image'] + \".jpg\", training_dir + new_name + \".jpg\")\n",
    "\n",
    "    header = ['image', 'label']\n",
    "\n",
    "    # saving training csv\n",
    "    training_out = np.column_stack((training_images, training_labels))\n",
    "    training_out = np.row_stack((header, training_out))\n",
    "    np.savetxt(destination_dir + 'training_labels.csv', training_out, delimiter=',', fmt='%s')\n",
    "\n",
    "    # saving validation csv\n",
    "    validation_out = np.column_stack((validation_images, validation_labels))\n",
    "    validation_out = np.row_stack((header, validation_out))\n",
    "    np.savetxt(destination_dir + 'validation_labels.csv', validation_out, delimiter=',', fmt='%s')\n",
    "\n",
    "    # saving testing csv\n",
    "    test_out = np.column_stack((test_images, test_labels))\n",
    "    test_out = np.row_stack((header, test_out))\n",
    "    np.savetxt(destination_dir + 'test_labels.csv', test_out, delimiter=',', fmt='%s')\n",
    "\n",
    "def process_and_save_image(source_path,destination_path):\n",
    "    img = resize(source_path, 224)\n",
    "    img.save(destination_path, quality=97)\n",
    "\n",
    "def resize(fname, target_size):\n",
    "    # print('Processing image: %s' % fname)\n",
    "    img = Image.open(fname)\n",
    "    resized = img.resize([target_size, target_size])\n",
    "    return resized\n",
    "\n",
    "#calling method\n",
    "create_tefla_data(sourceDir,destDir,validation_data_percentage,test_data_percentage,categories)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.7.5 locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.7.5 locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.7.5 locally\n",
      "2017-05-06 12:06:37,397 - INFO - Using parallel training iterator\n",
      "2017-05-06 12:06:37,923 - INFO - Config:\n",
      "2017-05-06 12:06:37,925 - INFO - {'aug_params': {'allow_stretch': True,\n",
      "                'do_flip': False,\n",
      "                'rotation_range': (-5, 5),\n",
      "                'shear_range': (0, 0),\n",
      "                'translation_range': (-2, 2),\n",
      "                'zoom_range': (0.9523809523809523, 1.05)},\n",
      " 'batch_size_test': 128,\n",
      " 'batch_size_train': 128,\n",
      " 'classification': True,\n",
      " 'iterator_type': 'parallel',\n",
      " 'lr_policy': StepDecayPolicy(schedule={0: 0.001, 15: 0.0001}),\n",
      " 'name': 'mnist_cnf',\n",
      " 'num_epochs': 30,\n",
      " 'optimizer': <tensorflow.python.training.adam.AdamOptimizer object at 0x7facf077b3d0>,\n",
      " 'standardizer': <tefla.da.standardizer.SamplewiseStandardizer object at 0x7facf077b350>,\n",
      " 'summary_every': 5,\n",
      " 'validation_scores': [('validation accuracy',\n",
      "                        <function accuracy_wrapper at 0x7facf78b5050>),\n",
      "                       ('validation kappa',\n",
      "                        <function kappa_wrapper at 0x7facf78b4d70>)]}\n",
      "2017-05-06 12:06:37,925 - INFO - Data: #Training images: 1412\n",
      "2017-05-06 12:06:37,925 - INFO - Data: #Classes: 23\n",
      "2017-05-06 12:06:37,925 - INFO - Data: Class frequencies: [(0, 58), (1, 64), (3, 0), (4, 306), (5, 323), (6, 24), (7, 8), (8, 13), (9, 9), (10, 15), (11, 8), (12, 19), (13, 43), (14, 18), (15, 24), (16, 81), (17, 32), (18, 86), (19, 27), (20, 164), (21, 39), (22, 6), (23, 37)]\n",
      "2017-05-06 12:06:37,926 - INFO - Data: Class balance weights: [  24.34482759   22.0625               inf    4.61437908    4.37151703\n",
      "   58.83333333  176.5         108.61538462  156.88888889   94.13333333\n",
      "  176.5          74.31578947   32.8372093    78.44444444   58.83333333\n",
      "   17.43209877   44.125        16.41860465   52.2962963     8.6097561\n",
      "   36.20512821  235.33333333   38.16216216  176.5       ]\n",
      "2017-05-06 12:06:37,927 - INFO - Data: #Validation images: 148\n",
      "2017-05-06 12:06:37,927 - INFO - Max epochs: 30\n",
      "2017-05-06 12:06:37,927 - INFO - \n",
      "---Trainable vars in model:\n",
      "2017-05-06 12:06:37,927 - INFO - conv1_1/biases:0 (32,)\n",
      "2017-05-06 12:06:37,927 - INFO - conv1_1/prelu/alpha:0 (32,)\n",
      "2017-05-06 12:06:37,927 - INFO - conv1_1/weights:0 (3, 3, 3, 32)\n",
      "2017-05-06 12:06:37,928 - INFO - conv1_2/biases:0 (32,)\n",
      "2017-05-06 12:06:37,928 - INFO - conv1_2/prelu/alpha:0 (32,)\n",
      "2017-05-06 12:06:37,928 - INFO - conv1_2/weights:0 (3, 3, 32, 32)\n",
      "2017-05-06 12:06:37,928 - INFO - fc1/biases:0 (128,)\n",
      "2017-05-06 12:06:37,928 - INFO - fc1/prelu/alpha:0 (128,)\n",
      "2017-05-06 12:06:37,928 - INFO - fc1/weights:0 (394272, 128)\n",
      "2017-05-06 12:06:37,929 - INFO - logits/biases:0 (24,)\n",
      "2017-05-06 12:06:37,929 - INFO - logits/weights:0 (128, 24)\n",
      "2017-05-06 12:06:37,929 - INFO - \n",
      "---Non Trainable vars in model:\n",
      "2017-05-06 12:06:37,929 - INFO - beta1_power:0 ()\n",
      "2017-05-06 12:06:37,929 - INFO - beta2_power:0 ()\n",
      "2017-05-06 12:06:37,930 - INFO - conv1_1/biases/Adam:0 (32,)\n",
      "2017-05-06 12:06:37,930 - INFO - conv1_1/biases/Adam_1:0 (32,)\n",
      "2017-05-06 12:06:37,930 - INFO - conv1_1/prelu/alpha/Adam:0 (32,)\n",
      "2017-05-06 12:06:37,930 - INFO - conv1_1/prelu/alpha/Adam_1:0 (32,)\n",
      "2017-05-06 12:06:37,930 - INFO - conv1_1/weights/Adam:0 (3, 3, 3, 32)\n",
      "2017-05-06 12:06:37,930 - INFO - conv1_1/weights/Adam_1:0 (3, 3, 3, 32)\n",
      "2017-05-06 12:06:37,931 - INFO - conv1_2/biases/Adam:0 (32,)\n",
      "2017-05-06 12:06:37,931 - INFO - conv1_2/biases/Adam_1:0 (32,)\n",
      "2017-05-06 12:06:37,931 - INFO - conv1_2/prelu/alpha/Adam:0 (32,)\n",
      "2017-05-06 12:06:37,931 - INFO - conv1_2/prelu/alpha/Adam_1:0 (32,)\n",
      "2017-05-06 12:06:37,931 - INFO - conv1_2/weights/Adam:0 (3, 3, 32, 32)\n",
      "2017-05-06 12:06:37,931 - INFO - conv1_2/weights/Adam_1:0 (3, 3, 32, 32)\n",
      "2017-05-06 12:06:37,931 - INFO - fc1/biases/Adam:0 (128,)\n",
      "2017-05-06 12:06:37,932 - INFO - fc1/biases/Adam_1:0 (128,)\n",
      "2017-05-06 12:06:37,932 - INFO - fc1/prelu/alpha/Adam:0 (128,)\n",
      "2017-05-06 12:06:37,932 - INFO - fc1/prelu/alpha/Adam_1:0 (128,)\n",
      "2017-05-06 12:06:37,932 - INFO - fc1/weights/Adam:0 (394272, 128)\n",
      "2017-05-06 12:06:37,932 - INFO - fc1/weights/Adam_1:0 (394272, 128)\n",
      "2017-05-06 12:06:37,932 - INFO - learning_rate:0 ()\n",
      "2017-05-06 12:06:37,933 - INFO - logits/biases/Adam:0 (24,)\n",
      "2017-05-06 12:06:37,933 - INFO - logits/biases/Adam_1:0 (24,)\n",
      "2017-05-06 12:06:37,933 - INFO - logits/weights/Adam:0 (128, 24)\n",
      "2017-05-06 12:06:37,933 - INFO - logits/weights/Adam_1:0 (128, 24)\n",
      "2017-05-06 12:06:37,933 - INFO - \n",
      "---Local vars in model:\n",
      "2017-05-06 12:06:37,933 - INFO - Total number of params:\n",
      "2017-05-06 12:06:37,934 - INFO - {'Local': 0, 'Non Trainable': 100960755.0, 'Trainable': 50480376}\n",
      "2017-05-06 12:06:37,934 - INFO - \n",
      "Model layer output shapes:\n",
      "2017-05-06 12:06:37,934 - INFO - inputs - (?, 224, 224, 3)\n",
      "2017-05-06 12:06:37,934 - INFO - conv1_1 - (?, 224, 224, 32)\n",
      "2017-05-06 12:06:37,934 - INFO - conv1_2 - (?, 224, 224, 32)\n",
      "2017-05-06 12:06:37,934 - INFO - pool1 - (?, 111, 111, 32)\n",
      "2017-05-06 12:06:37,934 - INFO - dropout1 - (?, 111, 111, 32)\n",
      "2017-05-06 12:06:37,935 - INFO - fc1 - (?, 128)\n",
      "2017-05-06 12:06:37,935 - INFO - dropout2 - (?, 128)\n",
      "2017-05-06 12:06:37,935 - INFO - logits - (?, 24)\n",
      "2017-05-06 12:06:37,935 - INFO - predictions - (?, 24)\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE3 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
      "W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
      "modprobe: FATAL: Module nvidia-uvm not found.\n",
      "E tensorflow/stream_executor/cuda/cuda_driver.cc:509] failed call to cuInit: CUDA_ERROR_UNKNOWN\n",
      "I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:145] kernel driver does not appear to be running on this host (ip-172-31-34-116): /proc/driver/nvidia/version does not exist\n",
      "2017-05-06 12:06:38,828 - INFO - Initial learning rate: 0.001000 \n",
      "2017-05-06 12:10:48,589 - INFO - Epoch 1 [(1412, 148) images,  249.5s]: t-loss: 101.926, v-loss: 32.263, t-loss/v-loss: 3.159, validation accuracy: 0.2905, validation kappa: 0.5075\n",
      "2017-05-06 12:10:53,716 - INFO - Next epoch learning rate: 0.001000 \n",
      "2017-05-06 12:14:45,149 - INFO - Epoch 2 [(1412, 148) images,  231.4s]: t-loss: 19.580, v-loss: 11.744, t-loss/v-loss: 1.667, validation accuracy: 0.5811, validation kappa: 0.6693\n",
      "2017-05-06 12:14:50,138 - INFO - Next epoch learning rate: 0.001000 \n",
      "2017-05-06 12:18:42,680 - INFO - Epoch 3 [(1412, 148) images,  232.5s]: t-loss: 11.218, v-loss: 9.831, t-loss/v-loss: 1.141, validation accuracy: 0.6216, validation kappa: 0.7293\n",
      "2017-05-06 12:18:47,797 - INFO - Next epoch learning rate: 0.001000 \n",
      "2017-05-06 12:22:52,317 - INFO - Epoch 4 [(1412, 148) images,  244.5s]: t-loss: 9.636, v-loss: 5.538, t-loss/v-loss: 1.740, validation accuracy: 0.6149, validation kappa: 0.7215\n",
      "2017-05-06 12:22:57,450 - INFO - Next epoch learning rate: 0.001000 \n",
      "2017-05-06 12:26:52,931 - INFO - Epoch 5 [(1412, 148) images,  235.5s]: t-loss: 8.674, v-loss: 8.010, t-loss/v-loss: 1.083, validation accuracy: 0.6284, validation kappa: 0.6305\n",
      "2017-05-06 12:26:57,896 - INFO - Next epoch learning rate: 0.001000 \n",
      "2017-05-06 12:30:54,060 - INFO - Epoch 6 [(1412, 148) images,  236.2s]: t-loss: 6.591, v-loss: 6.785, t-loss/v-loss: 0.971, validation accuracy: 0.6216, validation kappa: 0.7342\n",
      "2017-05-06 12:30:59,304 - INFO - Next epoch learning rate: 0.001000 \n",
      "2017-05-06 12:34:46,272 - INFO - Epoch 7 [(1412, 148) images,  227.0s]: t-loss: 4.988, v-loss: 6.154, t-loss/v-loss: 0.811, validation accuracy: 0.6149, validation kappa: 0.7973\n",
      "2017-05-06 12:34:51,393 - INFO - Next epoch learning rate: 0.001000 \n"
     ]
    }
   ],
   "source": [
    "!python -m tefla.train --model examples/mnist/mnist_model.py.py --training_cnf examples/mnist/mnist_cnf.py --data_dir /home/ec2-user/data/clothing/koovs_tefla_ready/  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcublas.so.7.5 locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcudnn.so.5 locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcufft.so.7.5 locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcuda.so.1 locally\n",
      "I tensorflow/stream_executor/dso_loader.cc:135] successfully opened CUDA library libcurand.so.7.5 locally\n",
      "Usage: train.py [OPTIONS]\n",
      "\n",
      "Options:\n",
      "  --model TEXT                   Relative path to model.\n",
      "  --training_cnf TEXT            Relative path to training config file.\n",
      "  --data_dir TEXT                Path to training directory.\n",
      "  --start_epoch INTEGER          Epoch number from which to resume training.\n",
      "                                 [default: 1]\n",
      "  --resume_lr TEXT               Learning rate for resumed training.\n",
      "  --weights_from TEXT            Path to initial weights file.\n",
      "  --weights_exclude_scopes TEXT  Scopes not to load from weights file.\n",
      "  --trainable_scopes TEXT        Scopes to train.\n",
      "  --clean                        Clean out training log and summary dir.\n",
      "  --visuals                      Visualize your training using various graphs.\n",
      "  --help                         Show this message and exit.\n"
     ]
    }
   ],
   "source": [
    "!python -m tefla.train --help\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
